from pyspark.sql.functions import col
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkExercise") \
    .getOrCreate()

applications_parquet_path = r"C:\Users\michalis.a\Desktop\spark\spark-interview-2-\spark-interview\applications_parquet"
loans_parquet_path = r"C:\Users\michalis.a\Desktop\spark\spark-interview-2-\spark-interview\loans_parquet"
sources_parquet_path = r"C:\Users\michalis.a\Desktop\spark\spark-interview-2-\spark-interview\sources_parquet"


print(f"Attempting to read all Parquet files")

try:
    applications_data = spark.read.parquet(applications_parquet_path)
    loans_data = spark.read.parquet(loans_parquet_path)
    sources_data = spark.read.parquet(sources_parquet_path)
except Exception as e:
    print(f"\nAn error occurred while reading Parquet files: {e}")
finally:  
    applications_data.createOrReplaceTempView("all_applications_data")
    loans_data.createOrReplaceTempView("all_loans_data")
    sources_data.createOrReplaceTempView("all_sources_data")

    # Exercise 1 - How many applications have been submitted from the beginning of the time.
    # I will check the count of distinct application ids 

    print(f"Execise 1")

    spark.sql("SELECT count(distinct(app_id)) as Sumbitted_loans FROM all_applications_data").show()

    # So the final answer is 400000
    # Extra, is we check how many distinct loan ids we have its only 2k
    # spark.sql("SELECT count(distinct(loan_id)) FROM all_applications_data").show()
   

    # ToDO show how much % is likely a loan to be approved and the ratio


    # Exercise 2 - How many applications have been submitted from the beginning of the time?
    # 2 approaches.
    # The first which I think is more correct is to join the applications with loans and get only the approved ones
    # The second is only to get the loans table because theoritika in there we only have the approved loans

    joined_applications_loans = applications_data.join(loans_data,on='loan_id',how='inner')
    joined_applications_loans.createOrReplaceTempView("applications_with_loans")

    # result = spark.sql("SELECT count(distinct(loan_id)) as total_loans FROM all_loans_data").show()
    # result = spark.sql("SELECT count(distinct(loan_id)) as total_approved_loans FROM applications_with_loans where status = 'approved'").show()

    #As we can see from here the second approach is not correct because we have 1 more loan in loans table
    #In my opinion is wrong and this loan should be removed, and only approved loans should be kept in that table since not all declined loans are presented there just 1
    #hence I think it's an outlier.

    #Answer for Exercise 2
    print(f"Execise 2")

    spark.sql("""
    SELECT 
        CONCAT(ROUND(AVG(commission), 2), ' â‚¬') AS Average_commission
    FROM applications_with_loans
    WHERE status = 'approved'
    """).show()


    #Exercise 3 which marketing sources are the first and second most popular for each loan type ?
    print(f"Execise 3")
    
    # Join the above result with sources on 'source_idd'
    full_joined_data = joined_applications_loans.alias("app_loans") \
    .join(sources_data.alias("src"),
          col("app_loans.source_id") == col("src.source_id"),
          how="left") \
    .select(
        col("app_loans.loan_name"),
        col("app_loans.source_id").alias("app_source_id"),  # rename to avoid ambiguity
        col("src.source_name")
    )

    # Register the joined DataFrame as a temp view for SQL queries
    full_joined_data.createOrReplaceTempView("applications_loans_sources")

    query = """
    WITH source_counts AS (
        SELECT
            loan_name,
            source_name,
            COUNT(*) AS application_count,
            ROW_NUMBER() OVER (PARTITION BY loan_name ORDER BY COUNT(*) DESC) as rank
        FROM applications_loans_sources
        GROUP BY loan_name, source_name
    )

    SELECT
        loan_name as Loan,
        MAX(CASE WHEN rank = 1 THEN source_name END) AS Most_Popular,
        MAX(CASE WHEN rank = 2 THEN source_name END) AS Second_Most_Popular
    FROM source_counts
    WHERE rank <= 2
    GROUP BY loan_name
    ORDER BY loan_name
    """

    spark.sql(query).show()

    #Exercise 4 - Provide a list that shows for each day what is the percentage of profit generated by each marketing source and to what percentage did they 
    #reach their daily target

    print(f"Execise 4")

    query = """
    WITH source_counts AS (
        SELECT
            source_id,
            date,
            sum(commission) as profit
        FROM applications_with_loans
        where status='approved'
        GROUP BY  source_id, date
        ORDER BY 2,3
    )

    SELECT 
    date as Date,
    source_name as Source,
    profit as Profit,
    CONCAT(Profit/daily_target*100,'%') as Daily_Target
        From source_counts sc
    LEFT JOIN all_sources_data asd on sc.source_id = asd.source_id
    ORDER BY 1,2
    """

    spark.sql(query).show()


